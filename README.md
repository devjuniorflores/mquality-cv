# Sistema de Monitoreo de Calidad

El objetivo de este proyecto es desarrollar un sistema de monitoreo de calidad basado en visiÃ³n por computadora, capaz de procesar, anotar, validar y preprocesar datos visuales (imÃ¡genes y videos) de manera automatizada.
El sistema integrarÃ¡ manejo de datos, anotaciÃ³n en formatos estÃ¡ndar (YOLO, COCO), validaciÃ³n y versionado con DVC, preprocesamiento y preparaciÃ³n para frameworks de deep learning, y orquestaciÃ³n de pipelines con Airflow.
Este flujo permitirÃ¡ tener datasets limpios, balanceados y listos para entrenamiento de modelos de detecciÃ³n y clasificaciÃ³n, asegurando reproducibilidad y escalabilidad.


## ğŸ“‚ Estructura inicial del proyecto
```plaintext
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Datos originales (no versionados en Git)
â”‚   â””â”€â”€ processed/        # Datos procesados
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ src/                  # CÃ³digo fuente
â”œâ”€â”€ .gitignore            # Ignorar data y archivos temporales
â”œâ”€â”€ README.md             # DocumentaciÃ³n principal
```
## Procesamiento y TransformaciÃ³n de Frames

En esta etapa del proyecto, el objetivo fue **preprocesar los frames extraÃ­dos** para prepararlos para futuras tareas de anÃ¡lisis y modelado.  

### ğŸ”¹ Actividades realizadas
1. **Lectura de imÃ¡genes con OpenCV**  
   - Uso de `cv2.imread()` para cargar imÃ¡genes desde `data/processed`.
   - VerificaciÃ³n de rutas y control de errores para asegurar la carga correcta.

2. **Redimensionamiento (Resize)**  
   - Uso de `cv2.resize()` para normalizar la resoluciÃ³n de los frames a un tamaÃ±o estÃ¡ndar (300x300 px).  
   - Esto facilita el procesamiento en modelos de visiÃ³n por computadora y reduce el costo computacional.

3. **Recorte (Crop)**  
   - SelecciÃ³n de una regiÃ³n especÃ­fica de interÃ©s (`img[y1:y2, x1:x2]`).  
   - El recorte permite centrar el anÃ¡lisis solo en el Ã¡rea relevante de cada frame, eliminando ruido visual.

4. **ExportaciÃ³n de imÃ¡genes procesadas**  
   - Guardado con `cv2.imwrite()` de los nuevos archivos (`resized_opencv.png` y `crop_opencv.png`) para uso en las siguientes fases del pipeline.

### Objetivo
Estandarizar y limpiar los frames para que los datos visuales estÃ©n listos para anÃ¡lisis, modelado o integraciÃ³n en pipelines de visiÃ³n por computadora.

## AnotaciÃ³n y Etiquetado de Datos 

En esta fase, nos enfocamos en la creaciÃ³n y validaciÃ³n de datasets anotados para tareas de detecciÃ³n de objetos, utilizando herramientas y formatos estÃ¡ndar del ecosistema de visiÃ³n por computadora.

### ğŸ”¹ Actividades realizadas

- InstalaciÃ³n y configuraciÃ³n de CVAT para anotaciÃ³n colaborativa.  
- ImportaciÃ³n y etiquetado de un conjunto de imÃ¡genes con bounding boxes y asignaciÃ³n de clases.  
- ExportaciÃ³n de las anotaciones en formatos **YOLO** y **COCO** para mÃ¡xima compatibilidad con frameworks de deep learning.  
- ValidaciÃ³n manual y automatizada de los archivos de anotaciÃ³n YOLO (`.txt`), asegurando que las coordenadas estÃ©n correctamente normalizadas entre 0 y 1.  
- ExploraciÃ³n de la estructura del archivo JSON de COCO, identificando y comprendiendo sus componentes clave: `images`, `annotations` y `categories`.

### ğŸ”¹ Resultados y aprendizajes clave

- ComprensiÃ³n profunda de los formatos de anotaciÃ³n y su impacto en el entrenamiento de modelos.  
- ImplementaciÃ³n de un script en Python para validar la calidad y consistencia de las anotaciones YOLO, reduciendo errores y mejorando la confiabilidad del dataset. (validacion_txtYolo.ipynb) 
- FamiliarizaciÃ³n con el flujo completo de anotaciÃ³n, exportaciÃ³n y validaciÃ³n, sentando las bases para la integraciÃ³n con pipelines de entrenamiento.

### Objetivo
Consolidar un flujo completo y confiable de anotaciÃ³n y validaciÃ³n de datos visuales, utilizando formatos estÃ¡ndar (YOLO y COCO), que permita generar datasets de alta calidad listos para entrenamiento de modelos de visiÃ³n por computadora. Esto asegura que las anotaciones sean precisas, consistentes y compatibles con mÃºltiples frameworks, facilitando el desarrollo eficiente y escalable de soluciones de detecciÃ³n y clasificaciÃ³n.

## ValidaciÃ³n de Dataset y ConfiguraciÃ³n de Remoto S3 con DVC

### ğŸ” Actividades realizadas
- VerificaciÃ³n de que las etiquetas coincidan con las imÃ¡genes y registro de las que no estÃ¡n etiquetadas en un `.csv`.
- DetecciÃ³n de imÃ¡genes **duplicadas** (mismo hash) y **corruptas** (archivos incompletos o ilegibles).
- ComprensiÃ³n de la estructura interna del cache de DVC (`.dvc/cache`), con subcarpetas basadas en los primeros 2 caracteres del hash.
- EjecuciÃ³n de `dvc push` para subir el dataset validado al bucket S3.

### ğŸ“š Conocimientos consolidados
- Uso de **hashes SHA-256** para identificar y deduplicar archivos.
- RelaciÃ³n entre cache local de DVC y almacenamiento remoto.

### Objetivo
Validar la integridad y consistencia del dataset, eliminando duplicados y detectando archivos corruptos, para luego configurar y subir el conjunto de datos a un almacenamiento remoto en Amazon S3 mediante DVC.

## Preprocesamiento

### ğŸ”¹ Actividades realizadas
1. **Resize uniforme** de las imÃ¡genes a dimensiones estÃ¡ndar (ej. `640x640`).
2. **NormalizaciÃ³n** de valores de pÃ­xeles a escala `[0,1]` (px/255).
3. **Data augmentation** con rotaciones, flips, y blur para aumentar diversidad.
4. **Balanceo de clases** mediante tÃ©cnicas de oversampling (creaciÃ³n de datos sintÃ©ticos).
5. **EstructuraciÃ³n final** del dataset en `data/processed/` manteniendo subcarpetas por clase.
6. **CreaciÃ³n de validador** para verificar integridad y conteo de imÃ¡genes por clase.
7. **DiscusiÃ³n sobre versionado con DVC**:
   - **Versionar:** `raw/` y `processed/`
   - **No versionar:** `temp/` (datos intermedios regenerables)

---

### ğŸ“Œ Conocimientos reforzados
- IdentificaciÃ³n y correcciÃ³n de **desbalance de clases**.
- Importancia de la **normalizaciÃ³n de datos** y escalas correctas (`[0,1]` o `[-1,1]`).
- Uso de **data augmentation** para robustecer el modelo.
- Buenas prÃ¡cticas en **gobernanza de datos** con DVC.
- Diferencia entre datos **intermedios** y datos **versionables**.

  **Objetivo del dÃ­a:**  
Implementar el pipeline de preprocesamiento y dejar el dataset listo para entrenamiento.

## EstructuraciÃ³n de datasets para frameworks

### ğŸ”¹ Actividades realizadas
- CreaciÃ³n de la estructura YOLO (`images/train`, `labels/train`, `images/val`, `labels/val`).
- DivisiÃ³n del dataset en 80% entrenamiento y 20% validaciÃ³n.
- VerificaciÃ³n de correspondencia entre imÃ¡genes y sus archivos `.txt` de anotaciones.
- Prueba de carga del dataset YOLO en un script para validar integridad.
- Registro del dataset COCO en Detectron2.
- ImplementaciÃ³n de un `CustomDataset` en PyTorch con `__len__` y `__getitem__`.
- Uso de `DataLoader` para iterar sobre batches y manejo de anotaciones con tamaÃ±os variables.

### ğŸ“Œ Conocimientos reforzados
- EstructuraciÃ³n y divisiÃ³n de datasets para entrenamiento y validaciÃ³n.
- Formatos de anotaciones YOLO y COCO y sus diferencias.
- Funcionamiento de `CustomDataset` en PyTorch.
- Uso de `DataLoader` para cargar datos por lotes.
- Manejo de bounding boxes y compatibilidad de dimensiones en tensores.

  **Objetivo del dÃ­a:**  
Organizar, convertir y probar datasets en formatos YOLO y COCO para asegurar su correcta carga y manipulaciÃ³n en PyTorch y Detectron2.

## ğŸ“‚ Estructura Final del proyecto
```plaintext
â”œâ”€â”€ .venv/                # Entorno Virtual
â”œâ”€â”€ airflow-dags/         
â”‚   â”œâ”€â”€ dags/             # DAGS
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Datos originales (no versionados en Git)
â”‚   â””â”€â”€ processed/        # Datos procesados
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ src/                  # CÃ³digo fuente
â”œâ”€â”€ .gitignore            # Ignorar data y archivos temporales
â”œâ”€â”€ README.md             # DocumentaciÃ³n principal
```


## OrquestaciÃ³n de pipelines de datos (Airflow)

### ğŸ”¹ Actividades realizadas
- InstalaciÃ³n y configuraciÃ³n inicial de **Apache Airflow** en entorno local.
- CreaciÃ³n de un **DAG** con pasos clave: descarga â†’ validaciÃ³n â†’ preprocesamiento â†’ exportaciÃ³n â†’ registro en DVC.
- Uso de **PythonOperator / TaskFlow API** para definir cada tarea.
- EjecuciÃ³n manual de un DAG con `airflow dags trigger`.
- SimulaciÃ³n de fallo en una tarea y verificaciÃ³n de reintento.
- ExploraciÃ³n de la **UI de Airflow**: monitoreo de logs, dependencias y estado de ejecuciÃ³n.

### ğŸ“Œ Conocimientos reforzados
- Concepto de **orquestaciÃ³n de pipelines**: dividir procesos complejos en tareas pequeÃ±as y manejables.
- Importancia de los **DAGs** para estructurar dependencias y orden en el flujo de datos.
- Manejo de **reintentos** y tolerancia a fallos en pipelines.
- IntegraciÃ³n de **Airflow + DVC** para versionar datasets dentro del flujo de orquestaciÃ³n.
- Buenas prÃ¡cticas: logs centralizados y monitoreo en la UI de Airflow.

### ğŸ¯ Objetivo del dÃ­a
Automatizar un pipeline de datos completo, asegurando que cada etapa (desde la descarga hasta el versionado) se ejecute de forma ordenada, reproducible y con capacidad de recuperaciÃ³n ante errores.
